
services:
  # Zookeeper - Necessario per Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log

  # Kafka - Message Broker per il streaming dei dati
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      # Configurazioni per ottimizzare le performance
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    volumes:
      - kafka-data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Logstash - Data Ingestion Engine
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    hostname: logstash
    container_name: logstash
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "5044:5044"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: "-Xmx1g -Xms1g"
    env_file:
      - .env
    volumes:
      - ./config/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
      - ./config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - logstash-data:/usr/share/logstash/data
      - ./logs:/usr/share/logstash/output_logs
    command: logstash -f /usr/share/logstash/pipeline/logstash.conf --config.reload.automatic
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9600/_node/stats || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Elasticsearch - Per archiviazione e analisi dei dati storici
  # Elasticsearch - Per archiviazione e analisi dei dati storici (Configurazione Leggera)
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    hostname: elasticsearch
    container_name: elasticsearch
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"  # Ridotto da 1g a 512m
      - cluster.name=portfolio-risk-cluster
      - node.name=portfolio-risk-node
      - bootstrap.memory_lock=false  # Disabilita memory lock
      - action.auto_create_index=true
      - cluster.initial_master_nodes=portfolio-risk-node
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    deploy:
      resources:
        limits:
          memory: 1G  # Limite memoria container
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Kibana - Interfaccia web per Elasticsearch (opzionale)
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    hostname: kibana
    container_name: kibana
    depends_on:
      elasticsearch:
        condition: service_healthy
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - xpack.security.enabled=false
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Spark - Stream Processing Engine
  spark-master:
    image: bitnami/spark:3.5.0
    hostname: spark-master
    container_name: spark-master
    ports:
      - "8081:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master Port
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    volumes:
      - ./scripts/spark:/opt/bitnami/spark/jobs
      - ./data:/opt/spark/data
      - spark-logs:/opt/bitnami/spark/logs
    command: >
      bash -c "
        pip install numpy scikit-learn pandas &&
        /opt/bitnami/scripts/spark/entrypoint.sh /opt/bitnami/scripts/spark/run.sh
      "

  spark-worker:
    image: bitnami/spark:3.5.0
    hostname: spark-worker
    container_name: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8082:8081"  # Spark Worker Web UI
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    volumes:
      - ./scripts/spark:/opt/bitnami/spark/jobs
      - ./data:/opt/spark/data
      - spark-logs:/opt/bitnami/spark/logs
    command: >
      bash -c "
        pip install numpy scikit-learn pandas &&
        /opt/bitnami/scripts/spark/entrypoint.sh /opt/bitnami/scripts/spark/run.sh
      "
  
  # Kafka UI - Interfaccia web per monitorare Kafka (opzionale ma utile)
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181

volumes:
  zookeeper-data:
  zookeeper-logs:
  kafka-data:
  logstash-data:
  spark-logs:
  elasticsearch-data:

networks:
  default:
    name: portfolio-risk-network