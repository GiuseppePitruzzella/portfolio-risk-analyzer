<br />
<div align="center">
    <a href="https://github.com/GiuseppePitruzzella/portfolio-risk-analyzer">
        <img src="assets/images/logo.png" alt="Logo" width="300"> </a>

    <h3 align="center">Real-Time Portfolio Risk Sentinel</h3>

    <p align="center">
        A real-time analysis pipeline to monitor and manage ETF portfolio risk.
        <br />
        <a href="https://github.com/GiuseppePitruzzella/portfolio-risk-analyzer/issues">Report a Bug</a>
        Â·
        <a href="https://github.com/GiuseppePitruzzella/portfolio-risk-analyzer/issues">Request a Feature</a>
    </p>
</div>

---

## ğŸ“˜ Project Overview

This project aims to provide individual investors with an advanced tool for **real-time ETF portfolio risk management**. Leveraging a robust architecture based on **Logstash, Kafka, Spark, Elasticsearch, and Grafana**, the application continuously monitors portfolio positions, calculates risk metrics such as **Value at Risk (VaR)** and **drawdown**, and generates automatic alerts when user-defined risk limits are exceeded. The goal is to enable faster, more informed decisions to protect invested capital.

---

## ğŸ“ˆ Pipeline Architecture

The core of this "Risk Sentinel" is a scalable and resilient data pipeline designed for real-time processing:

* **Logstash:** Ingests market data (ETF prices) and portfolio position information from various sources.
* **Kafka:** Acts as a high-speed distributed message broker, ensuring resilience and data stream distribution.
* **Spark:** The distributed computation engine that processes streaming data to calculate VaR, drawdown, and perform stress tests on positions.
* **Elasticsearch:** Stores calculated risk data and alerts, enabling fast queries and historical archiving.
* **Grafana:** Provides interactive dashboards for real-time risk metric visualization and custom alert configuration.
* **REST/WebSocket API (future):** For integration with external applications (e.g., mobile, trading desk), allowing real-time data access.

---

## ğŸ§ª Development Environment

The entire development environment and pipeline services are orchestrated via **Docker Compose**, ensuring reproducibility and easy setup.

### Prerequisites

Make sure you have installed:
* **Docker Desktop** (includes Docker Engine and Docker Compose)
* **Git**

### Local Setup

1.  **Clone the repository:**
        ```bash
        git clone https://github.com/GiuseppePitruzzella/portfolio-risk-analyzer.git
        cd portfolio-risk-analyzer
        ```
2.  **Configure Docker services:**
        The `docker-compose.yml` file defines all required services (Logstash, Kafka, Spark, Elasticsearch, Grafana). This file will be populated in upcoming project phases.
        ```bash
        # This command will be added once docker-compose.yml is defined
        # docker-compose up -d
        ```
3.  **Install Python dependencies:**
        Python dependencies for Spark applications and any API services will be listed in their respective `requirements.txt` files.
        ```bash
        # For main project dependencies (if present)
        pip install -r requirements.txt
        # For API-specific dependencies (if implemented)
        # pip install -r api/requirements.txt
        ```

---

## ğŸ“ Project Structure

````

portfolio-risk-analyzer/
â”œâ”€â”€ .gitignore               # File to ignore files and folders in Git
â”œâ”€â”€ README.md                # Project description and instructions
â”œâ”€â”€ requirements.txt         # General Python dependencies for the project
â”œâ”€â”€ docker-compose.yml       # Docker service definitions (Kafka, Spark, ES, Grafana, Logstash)
â”œâ”€â”€ docker/                  # Folder for custom Dockerfiles and configurations
â”‚   â”œâ”€â”€ logstash/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ config/          # Logstash configuration (pipelines, inputs, filters, outputs)
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ scripts/         # Startup scripts for Spark workers (if needed)
â”œâ”€â”€ spark_jobs/              # Source code for Spark applications
â”‚   â”œâ”€â”€ real_time_risk_calc.py # Main Spark script for calculations
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ risk_calculations.py # Modular functions for VaR, Drawdown, Stress-Test
â”œâ”€â”€ api/                     # Microservice for REST/WebSocket API (optional, e.g., Flask/FastAPI)
â”‚   â”œâ”€â”€ app.py               # Main API logic
â”‚   â””â”€â”€ requirements.txt     # API-specific dependencies
â”‚   â””â”€â”€ Dockerfile           # Dockerfile for the API service
â”œâ”€â”€ config/                  # Generic configuration files (e.g., default risk limits)
â”‚   â””â”€â”€ app_config.json
â”œâ”€â”€ data/                    # Folder for sample data, non-volatile historical data
â”‚   â””â”€â”€ historical_etf_prices.csv # Historical data for training/backtesting
â”‚   â””â”€â”€ portfolio_positions.csv   # Example portfolio positions
â”œâ”€â”€ docs/                    # Additional documentation (architecture, algorithms)
â””â”€â”€ scripts/                 # Utility scripts (setup, test, deploy)
â””â”€â”€ setup_kafka_topics.sh

```

---

## ğŸ”’ `.gitignore` Strategy

To keep the repository clean and prevent accidental commits of unnecessary or sensitive files:

* Files generated during execution (logs, temporary outputs).
* Downloaded Python dependencies (`venv`, `__pycache__`).
* Sensitive or environment-specific configuration files (`.env`).
* Large data files not intended for version control (e.g., full historical datasets, if too large).
* OS-specific system files (e.g., `.DS_Store`).

---

## ğŸ“Š Historical Data for "Training"

To calculate risk metrics such as historical VaR and for future backtesting or stress tests, **historical ETF price data** is required.

You can obtain this data from several sources:

* **Free Sources:** Yahoo Finance (via CSV download or Python libraries like `yfinance`), Investing.com, or public data from some stock exchanges. These are ideal for getting started.
* **Paid (Professional) Sources:** For greater reliability, granularity (intraday/tick-by-tick data), and coverage, consider paid API services like Alpha Vantage, Twelve Data, IEX Cloud, or, for institutional use, Bloomberg/Refinitiv.

Historical data will be loaded and processed by Spark to derive the necessary risk parameters, which will then be used in real-time calculations.

---

## ğŸ“¬ Contacts

[Giueppe Pitruzzella] â€“ [@GiuseppePitruzzella](https://github.com/GiuseppePitruzzella)
Project Repository â€“ [portfolio-risk-analyzer](https://github.com/GiuseppePitruzzella/portfolio-risk-analyzer)

